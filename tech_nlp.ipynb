{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edff4180",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "id": "ddd1e084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:46:45.708575Z",
     "start_time": "2024-10-09T08:46:36.150250Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "# 读取数据\n",
    "data = pd.read_excel('/content/tech.xlsx')\n",
    "\n",
    "# 将连续情感分数映射为离散类别\n",
    "# def map_score_to_category(score):\n",
    "#     if score < 0:\n",
    "#         return -1\n",
    "#     elif score > 0:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "# 将连续情感分数映射为离散类别\n",
    "def map_score_to_category(score):\n",
    "    if score < 0:\n",
    "        return -1\n",
    "    else :\n",
    "        return 1\n",
    "\n",
    "data['emotion_category'] = data['score'].apply(map_score_to_category)\n",
    "\n",
    "# 分割数据为训练集和测试集\n",
    "X = data['hd']\n",
    "y = data['emotion_category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 使用TF-IDF特征提取\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# 创建SVM模型\n",
    "svm_model = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# 训练模型\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 预测情感类别\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# 输出评估指标\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Accuracy:',accuracy_score(y_test, y_pred))\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/tech.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy_score\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# 读取数据\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/content/tech.xlsx\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# 将连续情感分数映射为离散类别\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# def map_score_to_category(score):\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m#     if score < 0:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m#         return 0\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# 将连续情感分数映射为离散类别\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmap_score_to_category\u001B[39m(score):\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\ml_pro\\lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001B[0m, in \u001B[0;36mread_excel\u001B[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001B[0m\n\u001B[0;32m    493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(io, ExcelFile):\n\u001B[0;32m    494\u001B[0m     should_close \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 495\u001B[0m     io \u001B[38;5;241m=\u001B[39m \u001B[43mExcelFile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m engine \u001B[38;5;129;01mand\u001B[39;00m engine \u001B[38;5;241m!=\u001B[39m io\u001B[38;5;241m.\u001B[39mengine:\n\u001B[0;32m    502\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    503\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine should not be specified when passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    505\u001B[0m     )\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\ml_pro\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001B[0m, in \u001B[0;36mExcelFile.__init__\u001B[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001B[0m\n\u001B[0;32m   1548\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxls\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1549\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1550\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[43minspect_excel_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1551\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n\u001B[0;32m   1552\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1553\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1554\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1555\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExcel file format cannot be determined, you must specify \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1556\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man engine manually.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1557\u001B[0m         )\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\ml_pro\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001B[0m, in \u001B[0;36minspect_excel_format\u001B[1;34m(content_or_path, storage_options)\u001B[0m\n\u001B[0;32m   1399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(content_or_path, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[0;32m   1400\u001B[0m     content_or_path \u001B[38;5;241m=\u001B[39m BytesIO(content_or_path)\n\u001B[1;32m-> 1402\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1403\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m   1404\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handle:\n\u001B[0;32m   1405\u001B[0m     stream \u001B[38;5;241m=\u001B[39m handle\u001B[38;5;241m.\u001B[39mhandle\n\u001B[0;32m   1406\u001B[0m     stream\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mD:\\anaconda3\\envs\\ml_pro\\lib\\site-packages\\pandas\\io\\common.py:882\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    874\u001B[0m             handle,\n\u001B[0;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    879\u001B[0m         )\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m--> 882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    883\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[0;32m    885\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/content/tech.xlsx'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "cb67ebbd",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959575e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_excel('/content/tech2.xlsx')\n",
    "\n",
    "# 将连续情感分数映射为二元类别\n",
    "def map_score_to_binary(score):\n",
    "    if score >= 0:\n",
    "        return 1  # 正面\n",
    "    else:\n",
    "        return 0  # 非正面\n",
    "\n",
    "data['emotion_binary'] = data['score'].apply(map_score_to_binary)\n",
    "# 分割数据为训练集和测试集\n",
    "X = data['hd']\n",
    "y = data['emotion_binary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 使用TF-IDF特征提取\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# 构建贝叶斯算法分类器\n",
    "mb = MultinomialNB(alpha=1)  # alpha 为可选项，默认 1.0，添加拉普拉修/Lidstone 平滑参数\n",
    "# 训练数据\n",
    "mb.fit(X_train_tfidf, y_train)\n",
    "# 预测数据\n",
    "y_predict = mb.predict(X_test_tfidf)\n",
    "#预测值与真实值展示\n",
    "# print('预测值：',y_predict)\n",
    "# print('真实值：',y_test)\n",
    "report = classification_report(y_test, y_predict) # X_test_tfidf, y_test\n",
    "print(report)\n",
    "\n",
    "mb.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f6e8b",
   "metadata": {},
   "source": [
    "## 分层采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd45f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import regularizers\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import importlib\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "df = pd.read_excel('tech.xlsx')\n",
    "df = df.sort_values('score')\n",
    "\n",
    "subset1, subset2, subset3 = np.split(df, [df[df['score'] < 0].index[-1]+1, df[df['score'] == 0].index[-1]+1])\n",
    "train1 = subset1.sample(frac=0.7)\n",
    "test1 = subset1.drop(train1.index)\n",
    "\n",
    "train2 = subset2.sample(frac=0.7)\n",
    "test2 = subset2.drop(train2.index)\n",
    "\n",
    "train3 = subset3.sample(frac=0.7)\n",
    "test3 = subset3.drop(train3.index)\n",
    "\n",
    "train_set = pd.concat([train1, train2, train3])\n",
    "test_set = pd.concat([test1, test2, test3])\n",
    "train_set = shuffle(train_set)\n",
    "test_set = shuffle(test_set)\n",
    "X_train = train_set['hd']\n",
    "y_train = train_set['score']\n",
    "X_test = test_set['hd']\n",
    "y_test = test_set['score']\n",
    "y_train = train_set['score']\n",
    "y_test = test_set['score']\n",
    "data=pd.concat([train_set, test_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a5693",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dac64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import importlib\n",
    "importlib.invalidate_caches()\n",
    "df = pd.read_excel('tech.xlsx') #/content/drive/MyDrive/数据集/数据集/大豆/gpt_soybean20180709-20180927 tech1 tech2 tech3\n",
    "\n",
    "data = shuffle(df)\n",
    "mean_score = data['score'].mean()\n",
    "# def map_score_to_category(score):\n",
    "#     if score < 0:\n",
    "#         return -1\n",
    "#     else :\n",
    "#         return 1\n",
    "# def map_score_to_category(score):\n",
    "#     if score < 0:\n",
    "#         return -1\n",
    "#     elif score > 0:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "#data['emotion_category'] = data['score'].apply(map_score_to_category)\n",
    "#data['score'].fillna(mean_score, inplace=True)\n",
    "scores =data['score']\n",
    "# 构建词汇表，将标题文本转换为数字序列\n",
    "tokenizer = tf.keras.layers.TextVectorization(output_mode='int')\n",
    "tokenizer.adapt(data['hd'])\n",
    "\n",
    "# 将标题文本转换为数字序列\n",
    "title_sequences = tokenizer(data['hd']).numpy()\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(title_sequences, np.array(scores), test_size=0.3, random_state=42) # 0.2 0.4\n",
    "\n",
    "# 构建模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.get_vocabulary()), output_dim=64, mask_zero=True),\n",
    "    tf.keras.layers.LSTM(64,kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# 编译模型\n",
    "model.compile(optimizer, loss='mean_squared_error')\n",
    "\n",
    "# 记录损失\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=2)\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "#output_file = 'lstm.txt'\n",
    "\n",
    "# with open(output_file, 'w') as f:\n",
    "#   f.write(str(history.history))\n",
    "# f.close()\n",
    "# 绘图\n",
    "# plt.plot(train_loss, label='Train Loss')\n",
    "# plt.plot(test_loss, label='Test Loss')\n",
    "# plt.title('LSTM Model Performance')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36892206",
   "metadata": {},
   "source": [
    "## ATT_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82965903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Layer, Embedding, LSTM, Dense\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W_q = self.add_weight(name=\"W_q\", shape=(input_shape[-1], input_shape[-1]), initializer=\"uniform\")\n",
    "        self.W_k = self.add_weight(name=\"W_k\", shape=(input_shape[-1], input_shape[-1]), initializer=\"uniform\")\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        q = tf.matmul(x, self.W_q)\n",
    "        k = tf.matmul(x, self.W_k)\n",
    "        v = x\n",
    "\n",
    "        attention_weights = tf.nn.softmax(tf.matmul(q, k, transpose_b=True), axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "# 定义新闻标题数据和对应的分数数据\n",
    "# df = pd.read_excel('tech.xlsx') # /content/drive/MyDrive/数据集/数据集/大豆/gpt_soybean20180709-20180927.xlsx\n",
    "# data = shuffle(df)\n",
    "#mean_score = data['score'].mean()\n",
    "#data['score'].fillna(mean_score, inplace=True)\n",
    "# def map_score_to_category(score):\n",
    "#     if score <= 0:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return 1\n",
    "# data['emotion_category'] = data['score'].apply(map_score_to_category)\n",
    "# scores = data['emotion_category']\n",
    "\n",
    "# 构建词汇表，将标题文本转换为数字序列\n",
    "tokenizer = tf.keras.layers.TextVectorization(output_mode='int')\n",
    "tokenizer.adapt(data['hd'])\n",
    "\n",
    "# 将标题文本转换为数字序列\n",
    "title_sequences = tokenizer(data['hd']).numpy()\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(title_sequences, np.array(scores), test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# 构建模型\n",
    "input_layer = tf.keras.layers.Input(shape=(title_sequences.shape[1],))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.get_vocabulary()), output_dim=64, mask_zero=True)(input_layer)\n",
    "lstm_layer = LSTM(64, kernel_regularizer=regularizers.l2(0.01),return_sequences=True)(embedding_layer)\n",
    "attention_layer = AttentionLayer()(lstm_layer)\n",
    "output_layer = Dense(1)(attention_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "# 定义精度指标函数\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    threshold = 0.4  # 自定义阈值 0.5 0.1 0.3\n",
    "    absolute_error = tf.abs(y_true - y_pred)\n",
    "    correct_predictions = tf.reduce_mean(tf.cast(absolute_error < threshold, tf.float32))\n",
    "    return correct_predictions\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[custom_accuracy])\n",
    "\n",
    "# 训练模型\n",
    "#model.fit(X_train, y_train, epochs=100, verbose=2)\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=2)\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "accuracy = history.history['custom_accuracy']  # 获取精度值\n",
    "\n",
    "# 绘图\n",
    "# plt.plot(train_loss, label='Train Loss')\n",
    "# plt.plot(test_loss, label='Test Loss')\n",
    "# plt.title('Attention-based LSTM Model Performance')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "#plt.savefig('myplot111.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12cc59",
   "metadata": {},
   "source": [
    "## seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3717c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取数据\n",
    "# data = pd.read_excel('tech.xlsx')\n",
    "\n",
    "# titles = data['hd']\n",
    "# scores = data['score']\n",
    "\n",
    "# 文本标记化\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(titles)\n",
    "sequences = tokenizer.texts_to_sequences(titles)\n",
    "\n",
    "\n",
    "max_len = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "labels = np.array(scores)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.3, random_state=42) #0.2 0.4\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 16\n",
    "latent_dim = 32\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(max_len,))\n",
    "encoder_embedding = tf.keras.layers.Embedding(input_dim=voicab_size, output_dim=embedding_dim, input_length=max_len)(encoder_inputs)\n",
    "encoder_lstm = tf.keras.layers.LSTM(latent_dim)(encoder_embedding)\n",
    "encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(latent_dim, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(max_len,))\n",
    "decoder_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)(decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(1, activation='linear')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "history = model.fit([X_train, X_train], y_train, epochs=50, batch_size=1, validation_split=0.2)\n",
    "\n",
    "# loss, mae = model.evaluate([X_test, X_test], y_test)\n",
    "# print(\"Test Mean Absolute Error:\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9a1f6",
   "metadata": {},
   "source": [
    "## transformer"
   ]
  },
  {
   "cell_type": "code",
   "id": "76739b2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T03:17:40.342810Z",
     "start_time": "2024-10-15T03:17:28.146154Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取数据\n",
    "# data = pd.read_excel('/content/tech.xlsx')\n",
    "\n",
    "# titles = data['hd']\n",
    "# scores = data['score']\n",
    "\n",
    "# # 文本标记化\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(titles)\n",
    "# sequences = tokenizer.texts_to_sequences(titles)\n",
    "\n",
    "# # 序列填充\n",
    "# max_len = max([len(seq) for seq in sequences])\n",
    "# padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# # 标签处理\n",
    "# labels = np.array(scores)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 32\n",
    "num_heads = 2\n",
    "dff = 32\n",
    "num_encoder_layers = 2\n",
    "\n",
    "# 位置编码\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# 建立Transformer模型\n",
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        attn_output = self.multi_head_attention(inputs, inputs, return_attention_scores=False)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        inputs = self.embedding(inputs)\n",
    "        inputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        inputs += self.pos_encoding[:, :seq_len, :]\n",
    "        inputs = self.dropout(inputs, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            inputs = self.enc_layers[i](inputs, training)\n",
    "        return inputs\n",
    "\n",
    "# 构建Transformer模型\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(1, activation='linear')\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        enc_output = self.encoder(inputs, training)\n",
    "        flattened_output = self.flatten(enc_output)\n",
    "        output = self.dense(flattened_output)\n",
    "        return output\n",
    "\n",
    "# 初始化并训练模型\n",
    "model = TransformerModel(\n",
    "    num_layers=num_encoder_layers,\n",
    "    d_model=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=vocab_size,\n",
    "    maximum_position_encoding=max_len,\n",
    ")    \n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "# print(\"Test Mean Absolute Error:\", mae)\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'padded_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 25\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# 读取数据\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# data = pd.read_excel('/content/tech.xlsx')\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# # 标签处理\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# labels = np.array(scores)\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(\u001B[43mpadded_sequences\u001B[49m, labels, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m     27\u001B[0m vocab_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mword_index) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     28\u001B[0m embedding_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m32\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'padded_sequences' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "179df628",
   "metadata": {},
   "source": [
    "## eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a70141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Author : zhany\n",
    "# @Time : 2019/03/20 \n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import jieba\n",
    "import synonyms\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "random.seed(2019)\n",
    "\n",
    "#停用词列表，默认使用哈工大停用词表\n",
    "f = open('stopwords/HIT_stop_words.txt')\n",
    "stop_words = list()\n",
    "for stop_word in f.readlines():\n",
    "    stop_words.append(stop_word[:-1])\n",
    "\n",
    "\n",
    "#考虑到与英文的不同，暂时搁置\n",
    "#文本清理\n",
    "'''\n",
    "import re\n",
    "def get_only_chars(line):\n",
    "    #1.清除所有的数字\n",
    "'''\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# 同义词替换\n",
    "# 替换一个语句中的n个单词为其同义词\n",
    "########################################################################\n",
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))     \n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0  \n",
    "    for random_word in random_word_list:          \n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)   \n",
    "            new_words = [synonym if word == random_word else word for word in new_words]   \n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n: \n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    new_words = sentence.split(' ')\n",
    "\n",
    "    return new_words\n",
    "\n",
    "def get_synonyms(word):\n",
    "    return synonyms.nearby(word)[0]\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# 随机插入\n",
    "# 随机在语句中插入n个词\n",
    "########################################################################\n",
    "def random_insertion(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    return new_words\n",
    "\n",
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    counter = 0    \n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return\n",
    "    random_synonym = random.choice(synonyms)\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "\n",
    "def random_swap(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "    return new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "    return new_words\n",
    "\n",
    "########################################################################\n",
    "# 随机删除\n",
    "# 以概率p删除语句中的词\n",
    "########################################################################\n",
    "def random_deletion(words, p):\n",
    "\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    if len(new_words) == 0:\n",
    "        rand_int = random.randint(0, len(words)-1)\n",
    "        return [words[rand_int]]\n",
    "\n",
    "    return new_words\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#EDA函数\n",
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    seg_list = \" \".join(seg_list)\n",
    "    words = list(seg_list.split())\n",
    "    num_words = len(words)\n",
    "\n",
    "    augmented_sentences = []\n",
    "    num_new_per_technique = int(num_aug/4)+1\n",
    "    n_sr = max(1, int(alpha_sr * num_words))\n",
    "    n_ri = max(1, int(alpha_ri * num_words))\n",
    "    n_rs = max(1, int(alpha_rs * num_words))\n",
    "\n",
    "    #print(words, \"\\n\")\n",
    "\n",
    "    \n",
    "    #同义词替换sr\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = synonym_replacement(words, n_sr)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    #随机插入ri\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_insertion(words, n_ri)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "    \n",
    "    #随机交换rs\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_swap(words, n_rs)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "   \n",
    "    #随机删除rd\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_deletion(words, p_rd)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "    \n",
    "    #print(augmented_sentences)\n",
    "    shuffle(augmented_sentences)\n",
    "\n",
    "    if num_aug >= 1:\n",
    "        augmented_sentences = augmented_sentences[:num_aug]\n",
    "    else:\n",
    "        keep_prob = num_aug / len(augmented_sentences)\n",
    "        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "    augmented_sentences.append(seg_list)\n",
    "\n",
    "    return augmented_sentences\n",
    "\n",
    "##\n",
    "#测试用例\n",
    "#eda(sentence=\"我们就像蒲公英，我也祈祷着能和你飞去同一片土地\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc1217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
